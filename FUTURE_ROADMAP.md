# Future Roadmap

This document outlines remaining architectural improvements that require significant effort but provide substantial value.

---

## 1. Async/Concurrent Processing

**Status:** âœ… Completed (v0.5.0)

**Why it matters:**
- Currently processes documents sequentially (N documents = N Ã— time)
- Network I/O wait time is wasted (LLM API calls take 2-30 seconds each)
- (Solved: Streaming responses are now supported, but processing is still serial)

**Value:** 3-5x speedup for multi-document workflows

**Implementation Plan:**
```
1. Add async HTTP client (aiohttp or httpx)
2. Convert LLMClient.generate_response() to async
3. Add asyncio.gather() for parallel document processing
4. Optional: Add streaming support for real-time output
```

**Estimated effort:** 2-3 days

---

## 2. Configuration File Support

**Status:** â¸ï¸ Deferred to Phase 3 (ä¸ GUI ä¸€èµ·å®ç°)

> å¯¹éæŠ€æœ¯ç”¨æˆ·æ“ä½œéš¾åº¦è¾ƒå¤§ï¼Œåœ¨æµè§ˆå™¨æ’ä»¶/Web UI ä¹‹å‰å®ç”¨æ€§æœ‰é™ã€‚

**Why it matters:**
- Users must pass CLI args every time
- No way to set project-specific defaults
- Environment variables are clunky for multiple settings

**Value:** Better UX, reproducible workflows

**Implementation Plan:**
```
1. Create config schema (YAML):
   ~/.editor-assistant/config.yml     # User defaults
   .editor-assistant.yml              # Project overrides

2. Add config loader in config/settings.py
3. Merge priority: CLI args > project > user > defaults
4. Document supported options
```

**Example config:**
```yaml
model: gpt-4
cache_enabled: true
rate_limit_per_minute: 60
output_dir: ./output
blocked_publishers:
  - nytimes.com
```

**Estimated effort:** 1 day

---

## 3. Dependency Injection / Loose Coupling

**Why it matters:**
- Hard to unit test (classes instantiate their dependencies directly)
- Cannot swap implementations (e.g., mock LLM for testing)
- Adding new providers requires modifying existing code

**Value:** Testability, extensibility, cleaner architecture

**Implementation Plan:**
```
1. Define interfaces/protocols for LLMClient, Converter, Validator
2. Pass dependencies via constructor (not self-instantiated)
3. Add simple factory or container for wiring
4. Update tests to use mocks
```

**Example:**
```python
# Before
class MDProcessor:
    def __init__(self, model_name):
        self.llm_client = LLMClient(model_name)  # tight coupling

# After
class MDProcessor:
    def __init__(self, llm_client: LLMClientProtocol):
        self.llm_client = llm_client  # injected
```

**Estimated effort:** 1-2 days

---

## 4. Plugin/Extension System (Partial)

**Status:** âš ï¸ Core Registry Pattern implemented (Task Architecture), External loading pending.

**Why it matters:**
- Adding new converters requires modifying core code
- Users cannot add custom prompts without forking
- No way to extend without touching internals

**Value:** User extensibility, cleaner separation

**Implementation Plan:**
```
1. Define plugin interfaces (Converter, Processor, Prompt) âœ… (TaskRegistry implemented)
2. Add registry pattern for dynamic registration âœ…
3. Scan plugin directories on startup (Pending)
4. Document plugin development (Pending)
```

**Example:**
```python
# plugins/my_converter.py
@register_converter("my-format")
class MyConverter(ConverterProtocol):
    def convert(self, path: str) -> MDArticle:
        ...
```

**Estimated effort:** 2-3 days

---

## 5. Persistence Layer (Partial)

**Status:** âœ… Core Implemented (Schema + History/Stats CLI), `resume` command pending.

**Why it matters:**

- Token usage only saved to text files
- No historical tracking or analytics
- Cannot resume interrupted processing

**Value:** Cost tracking, audit logs, checkpoint/resume

**Implementation Plan:**
```
1. Add SQLite database (lightweight, no server needed) âœ…
2. Schema: sessions, requests, token_usage, cache âœ…
3. Add CLI commands: `history`, `stats` âœ…, `resume` (Pending)
4. Optional: Export to CSV/JSON (Pending)
```

**Estimated effort:** 2-3 days

---

## Priority Order

| Priority | Item | Effort | Impact |
|----------|------|--------|--------|
| 1 | Resume capability & Export | 1 day | High (å®Œå–„ Persistence) |
| 2 | Tiered Pricing System | 1 day | Medium (æˆæœ¬å‡†ç¡®æ€§) |
| 3 | Dependency injection | 1-2 days | Medium (Testability) |
| 4 | Plugin system (å¤–éƒ¨åŠ è½½) | 2-3 days | Medium (Extensibility) |
| ~~5~~ | ~~Configuration file support~~ | ~~1 day~~ | ~~Deferred to Phase 3~~ |

> **Note**: Configuration file support å¯¹éæŠ€æœ¯ç”¨æˆ·æ“ä½œéš¾åº¦è¾ƒå¤§ï¼Œåœ¨ GUIï¼ˆå¦‚æµè§ˆå™¨æ’ä»¶ï¼‰ä¹‹å‰å®ç”¨æ€§æœ‰é™ï¼Œå·²æ¨è¿Ÿè‡³ Phase 3 ä¸å‰ç«¯ä¸€èµ·å®ç°ã€‚

---

## Completed Items (This Session)

âœ… Async/Concurrent Processing: `httpx` + `asyncio` refactor, 5x performance boost (v0.5.0)
âœ… Persistence Layer: SQLite storage, Schema, `history`/`stats` commands (Phase 1)
âœ… Performance: O(nÂ²) string fix, lazy loading, single-pass extraction
âœ… Maintenance: Typos, dead code, error handling, type hints
âœ… Scaling: Rate limiting, response caching
âœ… Validation: Content validation module
âœ… Code quality: Centralized constants, circular import fix

**Total: 16 issues resolved, 48 tests added**

---

## ğŸ”® Long-Term Vision (Phase 2+)

> ä»¥ä¸‹ä¸ºé•¿æœŸäº§å“æ„¿æ™¯ï¼Œåœ¨å½“å‰ TODO å’ŒåŸºç¡€æ¶æ„å®Œå–„åé€æ­¥å®æ–½ã€‚

### 6. Tiered Pricing System

**Why it matters:**
- è®¸å¤šæ¨¡å‹æŒ‰ä¸Šä¸‹æ–‡é•¿åº¦é˜¶æ¢¯è®¡è´¹ï¼ˆå¦‚ Gemini 3 Pro: <200k vs >200k tokensï¼‰
- å½“å‰ç»Ÿä¸€è®¡è´¹æ–¹å¼æ— æ³•å‡†ç¡®ä¼°ç®—æˆæœ¬

**Implementation Plan:**
```yaml
# llm_config.yml æ‰©å±•
models:
  gemini-3-pro:
    id: "gemini-3-pro-preview"
    pricing_tiers:
      - max_tokens: 200000
        input: 2.00
        output: 12.00
      - max_tokens: null  # unlimited
        input: 4.00
        output: 18.00
```

```python
# LLMClient è®¡è´¹é€»è¾‘
def calculate_cost(self, input_tokens, output_tokens):
    for tier in self.pricing_tiers:
        if tier.max_tokens is None or input_tokens <= tier.max_tokens:
            return (input_tokens * tier.input + output_tokens * tier.output) / 1_000_000
```

**Estimated effort:** 1 day

---

### 7. SciContent Benchmark Module

**Why it matters:**
- ç¼ºä¹é’ˆå¯¹ç§‘æŠ€å†…å®¹åˆ›ä½œ/ç§‘ç ”é˜…è¯»åœºæ™¯çš„ç³»ç»ŸåŒ–è¯„ä¼°æ¡†æ¶
- æ— æ³•å®šé‡æ¯”è¾ƒä¸åŒæ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°

**Core Capabilities:**
1. **ä»»åŠ¡è¦†ç›–**: å†™ä½œé£æ ¼ï¼ˆæ–°é—»/å­¦æœ¯/ç§‘æ™®ï¼‰ã€å­¦ç§‘ï¼ˆCS/Bio/Physicsï¼‰ã€è¯é¢˜
2. **è¯„ä¼°ç»´åº¦**:
   - ç”Ÿæˆè´¨é‡ï¼ˆäººå·¥ + è‡ªåŠ¨åŒ–æŒ‡æ ‡ï¼šBLEU/ROUGE/GPT-as-judgeï¼‰
   - ç”Ÿæˆé€Ÿåº¦ï¼ˆé¦– token æ—¶é—´ã€æ€»è€—æ—¶ï¼‰
   - æˆæœ¬æ•ˆç‡ï¼ˆ$/1K tokensã€$/taskï¼‰
   - ä¸€è‡´æ€§ï¼ˆå¤šæ¬¡ç”Ÿæˆçš„æ–¹å·®ï¼‰
3. **è¾“å‡ºæ ¼å¼**: JSON Linesï¼Œä¾¿äºåˆ†æå’Œå¯è§†åŒ–

**Architecture:**
```
benchmark/
â”œâ”€â”€ tasks/                    # ä»»åŠ¡å®šä¹‰
â”‚   â”œâ”€â”€ brief_generation.py
â”‚   â”œâ”€â”€ outline_generation.py
â”‚   â””â”€â”€ translation.py
â”œâ”€â”€ evaluators/               # è¯„ä¼°å™¨
â”‚   â”œâ”€â”€ quality.py           # GPT-as-judge, BLEU, ROUGE
â”‚   â”œâ”€â”€ latency.py           # TTFT, total time
â”‚   â””â”€â”€ cost.py              # Token-based cost
â”œâ”€â”€ datasets/                 # æµ‹è¯•æ•°æ®é›†
â”‚   â”œâ”€â”€ arxiv_cs/
â”‚   â”œâ”€â”€ arxiv_bio/
â”‚   â””â”€â”€ news_tech/
â”œâ”€â”€ runners/                  # è¿è¡Œå™¨
â”‚   â””â”€â”€ benchmark_runner.py
â””â”€â”€ reports/                  # ç»“æœè¾“å‡º
    â”œâ”€â”€ leaderboard.json
    â””â”€â”€ detailed_results.jsonl
```

**CLI:**
```bash
# è¿è¡Œ benchmark
editor-assistant benchmark --models gemini-3-flash,deepseek-r1 --tasks brief,outline

# æŸ¥çœ‹ç»“æœ
editor-assistant benchmark-report --format table
```

**Future Extension - Agentic Tasks:**
- è‡ªä¸»é€‰é¢˜ï¼ˆæ ¹æ®ç”¨æˆ·å…´è¶£/çƒ­ç‚¹è¶‹åŠ¿ï¼‰
- å¤šè½®è¿­ä»£ç”Ÿæˆï¼ˆæ ¹æ®è¯„ä¼°åé¦ˆè‡ªæˆ‘æ”¹è¿›ï¼‰
- å¤š agent åä½œï¼ˆç ”ç©¶ agent + å†™ä½œ agent + å®¡æ ¸ agentï¼‰

**Estimated effort:** 2-3 weeks

---

### 8. Interactive AI Assistant (SciEditor Assistant)

**Why it matters:**
- ç§‘æŠ€å†…å®¹ç¼–è¾‘éœ€è¦ human-in-the-loop å·¥ä½œæµ
- ç”¨æˆ·åé¦ˆæ˜¯å®è´µçš„ RLHF ä¿¡å·

**Core Capabilities:**
1. **è‡ªä¸»é€‰é¢˜**: åŸºäº RSS feedsã€arXivã€çƒ­ç‚¹è¶‹åŠ¿è‡ªåŠ¨æ¨èé€‰é¢˜
2. **å†…å®¹ç”Ÿæˆ**: æ ¹æ®ç”¨æˆ·è¦æ±‚ç”Ÿæˆåˆç¨¿
3. **åé¦ˆæ”¶é›†**:
   - é€‰é¢˜é€šè¿‡ç‡
   - äººå·¥ä¿®è®¢ diff
   - ç”¨æˆ·è¯„åˆ†
4. **åé¦ˆé—­ç¯**: åé¦ˆæ•°æ®ç”¨äº benchmark è¯„ä¼°æˆ– fine-tuning

**Data Schema:**
```sql
-- é€‰é¢˜è®°å½•
CREATE TABLE topics (
    id INTEGER PRIMARY KEY,
    source_url TEXT,
    suggested_at TIMESTAMP,
    status TEXT,  -- 'suggested', 'accepted', 'rejected', 'published'
    rejection_reason TEXT
);

-- ç”Ÿæˆè®°å½•
CREATE TABLE generations (
    id INTEGER PRIMARY KEY,
    topic_id INTEGER,
    model TEXT,
    prompt TEXT,
    raw_output TEXT,
    edited_output TEXT,  -- äººå·¥ä¿®è®¢å
    edit_distance INTEGER,
    user_rating INTEGER,  -- 1-5
    feedback_text TEXT
);
```

**Feedback â†’ Benchmark Integration:**
```python
# ä»ç”¨æˆ·åé¦ˆç”Ÿæˆ benchmark æ•°æ®
def export_feedback_to_benchmark(db_path: str) -> List[BenchmarkSample]:
    """
    Convert user feedback into benchmark evaluation samples.
    - Accepted topics with high ratings â†’ positive examples
    - Large edit distances â†’ areas for improvement
    """
    ...
```

**Estimated effort:** 3-4 weeks

---

### 9. Frontend Forms (Phase 3)

**Why it matters:**
- CLI å¯¹éæŠ€æœ¯ç”¨æˆ·ä¸å‹å¥½
- äº¤äº’å¼ç¼–è¾‘éœ€è¦ GUI

**Implementation Options:**

| å½¢æ€ | æŠ€æœ¯æ ˆ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|--------|------|------|
| **Web UI** | FastAPI + React/Vue | è·¨å¹³å°ã€æ˜“éƒ¨ç½² | éœ€è¦æœåŠ¡å™¨ |
| **Browser Extension** | Chrome Extension + Web Components | æ— ç¼é›†æˆæµè§ˆ | åŠŸèƒ½å—é™ |
| **Desktop App** | Electron / Tauri | ç¦»çº¿ä½¿ç”¨ã€å…¨åŠŸèƒ½ | åŒ…ä½“ç§¯å¤§ |
| **RSS Reader** | Tauri + SQLite | ä¸“æ³¨é˜…è¯»åœºæ™¯ | éœ€è¦ç»´æŠ¤è®¢é˜…æº |

**Recommended Approach:**
1. **Phase 3.1**: Web UI (FastAPI + Vue/React) - æ ¸å¿ƒåŠŸèƒ½ MVP
2. **Phase 3.2**: Browser Extension - å¤ç”¨ Web UI ç»„ä»¶ï¼Œæä¾›é¡µé¢å†…åŠ©æ‰‹
3. **Phase 3.3**: Desktop App (Tauri) - æ‰“åŒ… Web UIï¼Œæ·»åŠ ç¦»çº¿æ”¯æŒ

**Web UI Architecture:**
```
frontend/
â”œâ”€â”€ web/                      # Vue/React SPA
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ TopicSuggester.vue
â”‚   â”‚   â”œâ”€â”€ ContentEditor.vue
â”‚   â”‚   â”œâ”€â”€ FeedbackPanel.vue
â”‚   â”‚   â””â”€â”€ BenchmarkDashboard.vue
â”‚   â””â”€â”€ pages/
â”‚       â”œâ”€â”€ Editor.vue
â”‚       â”œâ”€â”€ Benchmark.vue
â”‚       â””â”€â”€ Settings.vue
â”œâ”€â”€ api/                      # FastAPI backend
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ topics.py
â”‚   â”‚   â”œâ”€â”€ generations.py
â”‚   â”‚   â””â”€â”€ benchmark.py
â”‚   â””â”€â”€ main.py
â””â”€â”€ extension/                # Chrome Extension
    â”œâ”€â”€ popup/
    â”œâ”€â”€ content-script/
    â””â”€â”€ background/
```

**Estimated effort:** 4-6 weeks (Web UI MVP)

---

## Updated Priority Order

| Phase | Item | Effort | Impact |
|-------|------|--------|--------|
| **1 (Current)** | Resume capability & Export | 1 day | High (å®Œå–„ Persistence) |
| **1** | Tiered pricing | 1 day | Medium (æˆæœ¬å‡†ç¡®æ€§) |
| **1** | Dependency injection | 1-2 days | Medium (Testability) |
| **2** | Benchmark module | 2-3 weeks | High (Product) |
| **2** | Interactive assistant (backend) | 3-4 weeks | High (Product) |
| **3** | Configuration file support | 1 day | High (ä¸ GUI é…åˆ) |
| **3** | Web UI | 4-6 weeks | High (Adoption) |
| **3** | Browser extension | 2 weeks | Medium (UX) |
| **3** | Desktop app | 2-3 weeks | Medium (Offline) |
