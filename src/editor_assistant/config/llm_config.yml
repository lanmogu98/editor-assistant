# This is the single source of truth for all LLM configuration.
# The structure is: provider -> provider_settings -> model_name -> model_details

deepseek:
  api_key_env_var: "VOLC_API_KEY"
  api_base_url: "https://ark.cn-beijing.volces.com/api/v3/chat/completions"
  temperature: 0.5
  max_tokens: 16000
  context_window: 128000
  pricing_currency: "CNY"
  models:
    deepseek-r1:
      id: "deepseek-r1-250120"
      pricing: { input: 4.00, output: 16.00 }
    deepseek-r1-latest:
      id: "deepseek-r1-250528"
      pricing: { input: 4.00, output: 16.00 }
    deepseek-v3:
      id: "deepseek-v3-241226"
      pricing: { input: 2.00, output: 8.00 }
    deepseek-v3-latest:
      id: "deepseek-v3-250324"
      pricing: { input: 2.00, output: 8.00 }

gemini:
  api_key_env_var: "GEMINI_API_KEY"
  api_base_url: "https://generativelanguage.googleapis.com/v1beta/models"
  temperature: 0.6
  max_tokens: 8192
  context_window: 200000
  pricing_currency: "USD"
  models:
    gemini-2.5-flash-lite:
      id: "gemini-2.5-flash-lite-preview-06-17"
      pricing: { input: 0.10, output: 0.40 } # USD per 1M tokens
    gemini-2.5-flash:
      id: "gemini-2.5-flash"
      pricing: { input: 0.30, output: 2.50 } # USD per 1M tokens
    gemini-2.5-pro:
      id: "gemini-2.5-pro"
      pricing: { input: 1.25, output: 10.00 } # USD per 1M tokens 