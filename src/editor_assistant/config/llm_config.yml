# |
#   This is the single source of truth for all LLM configuration.
#   The structure is: provider -> provider_settings -> model_name -> model_details
#   Users can add new models to the config file in the format demonstrated below.
#    - provider: the name of the LLM provider.
#    - provider_settings: the settings of the LLM provider.
#    - model_name: the name of the LLM model.
#    - model_details: the details of the LLM model.
#    - pricing: the pricing of the LLM model, per 1M tokens.
#    - pricing_currency: the currency of the pricing.
#    - max_tokens: the maximum number of tokens the model can generate.
#    - context_window: the maximum number of tokens the model can process.
#    - rate_limit (optional): per-provider rate limiting configuration
#        - min_interval_seconds: minimum time between requests (default: 0.5)
#        - max_requests_per_minute: max requests per minute, 0 = unlimited (default: 60)
#   
#   In default, and as a highly recommend practice, the API key of each model is 
#   stored in the environment variable.

#-------------------------------------------------------------------------------
# Shared endpoints for reuse across providers
#-------------------------------------------------------------------------------
_shared_endpoints:
  volcengine: &volcengine_endpoint "https://ark.cn-beijing.volces.com/api/v3/chat/completions"
  openrouter: &openrouter_endpoint "https://openrouter.ai/api/v1/chat/completions"


deepseek-volcengine:
  api_key_env_var: "DEEPSEEK_API_KEY_VOLC"
  api_base_url: *volcengine_endpoint
  temperature: 0.4
  max_tokens: 16000
  context_window: 128000
  pricing_currency: "짜"
  models:
    deepseek-v3.2:
      id: "deepseek-v3-2-251201"
      pricing: { input: 4.00, output: 6.00 }
    deepseek-r1:
      id: "deepseek-r1-250528"
      pricing: { input: 4.00, output: 12.00 }

# deepseek:
#   api_key_env_var: "DEEPSEEK_API_KEY"
#   api_base_url: "https://api.deepseek.com/chat/completions"
#   temperature: 0.4
#   max_tokens: 32000
#   context_window: 128000
#   pricing_currency: "$"
#   models:
#     deepseek-v3.2:
#       id: "deepseek-reasoner"
#       pricing: { input: 0.28, output: 0.42}

gemini:
  api_key_env_var: "GEMINI_API_KEY"
  api_base_url: "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions"
  temperature: 1.0  # Gemini 3 recommends default 1.0 to avoid looping issues
  max_tokens: 65536
  context_window: 1000000  # Gemini 3 supports 1M token input
  pricing_currency: "$"
  # Gemini 3 thinking configuration via OpenAI compatibility layer
  # If not set, model uses dynamic thinking (defaults to "high" with auto-adjustment)
  # Use CLI --thinking to override: low | medium | high | minimal
  # See: https://ai.google.dev/gemini-api/docs/gemini-3
  # request_overrides:
  #   reasoning_effort: "high"  # Uncomment to force specific level
  models:
    gemini-3-flash:
      id: "gemini-3-flash-preview"
      pricing: { input: 0.5, output: 3.00 } 
    gemini-3-pro:
      id: "gemini-3-pro-preview"
      pricing: { input: 2.00, output: 12.00 } # USD per 1M tokens

# Gemini Free Tier - uses separate API key with stricter rate limits
# Rate limits are conservative; update after testing with AI Studio data
gemini-free:
  api_key_env_var: "GEMINI_FT_API_KEY"
  api_base_url: "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions"
  temperature: 1.0
  max_tokens: 65536
  context_window: 1000000
  pricing_currency: "$"
  # Conservative rate limiting - update after testing
  rate_limit:
    min_interval_seconds: 12.0  # ~5 RPM, very conservative
    max_requests_per_minute: 5
  models:
    gemini-3-flash-free:
      id: "gemini-3-flash-preview"
      pricing: { input: 0.0, output: 0.0 }
    gemini-3-pro-free:
      id: "gemini-3-pro-preview"
      pricing: { input: 0.0, output: 0.0 } 

kimi-volcengine:
  api_key_env_var: "KIMI_API_KEY_VOLC"
  api_base_url: *volcengine_endpoint
  temperature: 0.5
  max_tokens: 32000
  context_window: 128000
  pricing_currency: "짜"
  models:
    kimi-k2:
      id: "kimi-k2-250905"
      pricing: {input: 4.00, output: 16.00}

qwen:
  api_key_env_var: "QWEN_API_KEY"
  api_base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
  temperature: 0.6
  max_tokens: 32768
  context_window: 995904
  pricing_currency: "짜"
  models:
    qwen-plus:
      id: "qwen-plus"
      pricing: {input: 0.80, output: 2.00} # with thinking model defaultly disabled
    qwen3-max-preview:
      id: "qwen3-max-preview"
      pricing: {input: 0.60, output: 24.00} 
    qwen3-max:
      id: "qwen3-max"
      pricing: {input: 0.60, output: 24.00} 

zhipu:
  api_key_env_var: "ZHIPU_API_KEY"
  api_base_url: "https://open.bigmodel.cn/api/paas/v4/chat/completions"
  temperature: 0.6
  max_tokens: 128000  # GLM-4.7 supports up to 128K output
  context_window: 200000  # GLM-4.7 supports 200K context
  pricing_currency: "$"
  models:
    glm-4.5:
      id: "glm-4.5"
      pricing: {input: 0.60, output: 2.20}
    glm-4.6:
      id: "glm-4.6"
      pricing: {input: 0.60, output: 2.20}
    glm-4.7:
      id: "glm-4.7"
      pricing: {input: 0.60, output: 2.20}

zhipu-openrouter:
  api_key_env_var: "ZHIPU_API_KEY_OPENROUTER"
  api_base_url: *openrouter_endpoint
  temperature: 0.6
  max_tokens: 131100  # GLM-4.7 max output on OpenRouter
  context_window: 200000  # GLM-4.7 supports 200K context
  pricing_currency: "$"
  # overides for the OpenRouter provider
  request_overrides:
    provider:
      only: ["z-ai"]
      allow_fallbacks: false
  models:
    glm-4.5-or:
      id: "z-ai/glm-4.5"
      pricing: {input: 0.38, output: 1.60}
    glm-4.6-or:
      id: "z-ai/glm-4.6"
      pricing: {input: 0.60, output: 2.00}
    glm-4.7-or:
      id: "z-ai/glm-4.7"
      pricing: {input: 0.60, output: 2.20}

doubao:
  api_key_env_var: "DOUBAO_API_KEY"
  api_base_url: *volcengine_endpoint
  temperature: 0.6
  max_tokens: 32000
  context_window: 256000
  pricing_currency: "짜"
  models:
    doubao-seed-1.6:
      id: "doubao-seed-1-6-250615"
      pricing: {input: 0.80, output: 2.00}

openai-openrouter:
  api_key_env_var: "OPENAI_API_KEY_OPENROUTER"
  api_base_url: *openrouter_endpoint
  temperature: 0.6
  max_tokens: 164000
  context_window: 128000
  pricing_currency: "$"
  models:
    gpt-4o-or:
      id: "openai/gpt-4o"
      pricing: {input: 2.5, output: 10.00}
    gpt-4.1-or:
      id: "openai/gpt-4.1"
      pricing: {input: 2.00, output: 8.00}
    gpt-5-or:
      id: "openai/gpt-5"
      pricing: {input: 1.25, output: 10.00}

anthropic-openrouter:
  api_key_env_var: "ANTHROPIC_API_KEY_OPENROUTER"
  api_base_url: *openrouter_endpoint
  temperature: 0.6
  max_tokens: 64000
  context_window: 200000
  pricing_currency: "$"
  # Example: per-provider rate limiting (optional, uses defaults if not specified)
  rate_limit:
    min_interval_seconds: 1.0  # Claude is expensive, slow down requests
    max_requests_per_minute: 30
  models:
    claude-sonnet-4-or:
      id: "anthropic/claude-sonnet-4"
      pricing: {input: 3.00, output: 15.00}