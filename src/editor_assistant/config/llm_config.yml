# |
#   This is the single source of truth for all LLM configuration.
#   The structure is: provider -> provider_settings -> model_name -> model_details
#   Users can add new models to the config file in the format demonstrated below.
#    - provider: the name of the LLM provider.
#    - provider_settings: the settings of the LLM provider.
#    - model_name: the name of the LLM model.
#    - model_details: the details of the LLM model.
#    - pricing: the pricing of the LLM model, per 1M tokens.
#    - pricing_currency: the currency of the pricing.
#    - max_tokens: the maximum number of tokens the model can generate.
#    - context_window: the maximum number of tokens the model can process.
#   
#   In default, and as a highly recommend practice, the API key of each model is 
#   stored in the environment variable.


deepseek:
  api_key_env_var: "DEEPSEEK_API_KEY"
  api_base_url: "https://ark.cn-beijing.volces.com/api/v3/chat/completions"
  temperature: 0.4
  max_tokens: 16000
  context_window: 128000
  pricing_currency: "¥"
  models:
    deepseek-v3.1:
      id: "deepseek-v3-1-250821"
      pricing: { input: 4.00, output: 12.00 }
    deepseek-r1:
      id: "deepseek-r1-250120"
      pricing: { input: 4.00, output: 16.00 }
    deepseek-r1-latest:
      id: "deepseek-r1-250528"
      pricing: { input: 4.00, output: 16.00 }
    deepseek-v3:
      id: "deepseek-v3-241226"
      pricing: { input: 2.00, output: 8.00 }
    deepseek-v3-latest:
      id: "deepseek-v3-250324"
      pricing: { input: 2.00, output: 8.00 }

gemini:
  api_key_env_var: "GEMINI_API_KEY"
  api_base_url: "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions"
  temperature: 0.5
  max_tokens: 65536
  context_window: 200000
  pricing_currency: "$"
  models:
    gemini-2.5-flash-lite:
      id: "gemini-2.5-flash-lite-preview-06-17"
      pricing: { input: 0.10, output: 0.40 } 
    gemini-2.5-flash:
      id: "gemini-2.5-flash"
      pricing: { input: 0.30, output: 2.50 } 
    gemini-2.5-pro:
      id: "gemini-2.5-pro"
      pricing: { input: 1.25, output: 10.00 } # USD per 1M tokens 

kimi:
  api_key_env_var: "KIMI_API_KEY"
  api_base_url: "https://ark.cn-beijing.volces.com/api/v3/chat/completions"
  temperature: 0.5
  max_tokens: 32000
  context_window: 128000
  pricing_currency: "¥"
  models:
    kimi-k2:
      id: "kimi-k2-250905"
      pricing: {input: 4.00, output: 16.00}

qwen:
  api_key_env_var: "QWEN_API_KEY"
  api_base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
  temperature: 0.6
  max_tokens: 32768
  context_window: 995904
  pricing_currency: "¥"
  models:
    qwen-plus:
      id: "qwen-plus"
      pricing: {input: 0.80, output: 2.00} # with thinking model defaultly disabled
    qwen-plus-latest:
      id: "qwen-plus-latest"
      pricing: {input: 0.80, output: 2.00} # with thinking model defaultly disabled

zhipu:
  api_key_env_var: "ZHIPU_API_KEY"
  api_base_url: "https://open.bigmodel.cn/api/paas/v4/chat/completions"
  temperature: 0.6
  max_tokens: 96000
  context_window: 128000
  pricing_currency: "¥"
  models:
    glm-4.5:
      id: "glm-4.5"
      pricing: {input: 0.80, output: 8.00}

zhipu-openrouter:
  api_key_env_var: "ZHIPU_API_KEY_OPENROUTER"
  api_base_url: "https://openrouter.ai/api/v1/chat/completions"
  temperature: 0.6
  max_tokens: 96000
  context_window: 131100
  pricing_currency: "$"
  # overides for the OpenRouter provider
  request_overrides:
    provider:
      only: ["z-ai"]
      allow_fallbacks: false
  models:
    glm-4.5-openrouter:
      id: "z-ai/glm-4.5"
      pricing: {input: 0.60, output: 2.20}

doubao:
  api_key_env_var: "DOUBAO_API_KEY"
  api_base_url: "https://ark.cn-beijing.volces.com/api/v3/chat/completions"
  temperature: 0.6
  max_tokens: 32000
  context_window: 256000
  pricing_currency: "¥"
  models:
    doubao-seed-1.6:
      id: "doubao-seed-1-6-250615"
      pricing: {input: 0.80, output: 2}